# 激活函数的导数

- [激活函数的导数](#激活函数的导数)
  - [Sigmoid activation function](#sigmoid-activation-function)
  - [Tanh activation function](#tanh-activation-function)
  - [ReLU](#relu)
  - [Leaky ReLU](#leaky-relu)

## Sigmoid activation function

$$
g(z) = \frac{1}{1 + e^{-z}}
\\
g^{\prime(z)} = \frac{d}{dz}g(z) = \frac{1}{1 + e^{-z}}(1 - \frac{1}{1 + e^{-z}}) = g(z)(1 - g(z))
$$

## Tanh activation function

$$
g(z) = tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
\\
g^{\prime(z)} = \frac{d}{dz}g(z) = 1 - (tanh(z))^{2} = 1 - (g(z))^{2}
$$

## ReLU

$$
g(z) = max(0, z)
\\
g^{\prime(z)} = \frac{d}{dz}g(z) = \begin{cases}
0 & if \quad z < 0
\\
1 & if \quad z > 0
\\
undefined & if \quad z = 0
\end{cases}
$$

注：通常在 $z = 0$ 的时候给定其导数 $1,0$；当然 $z=0$ 的情况很少

## Leaky ReLU

$$
g(z) = max(0.01z, z)
\\
g^{\prime(z)} = \frac{d}{dz}g(z) = \begin{cases}
0.01 & if \quad z < 0
\\
1 & if \quad z > 0
\\
undefined & if \quad z = 0
\end{cases}
$$

注：通常在 $z = 0$ 的时候给定其导数 $1,0.01$；当然 $z=0$ 的情况很少
