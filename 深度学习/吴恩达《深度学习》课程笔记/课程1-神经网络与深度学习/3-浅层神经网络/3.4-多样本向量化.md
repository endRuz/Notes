# 多样本向量化

在 [3.3 计算神经网络的输出](./3.3-计算神经网络的输出.md) 中了解到如何针对于单一的训练样本，在神经网络上计算出预测值。得到的公式如下：

$
z^{[1]} = (W^{[1]})^{T}a^{[0]} + b^{[1]}
\\
a^{[1]} = \sigma(z^{[1]})
\\
z^{[2]} = (W^{[2]})^{T}a^{[1]} + b^{[2]}
\\
a^{[2]} = \sigma(z^{[2]})
$

其中
$X = a^{[0]} = \begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}$
是单一样本。


在本小节中，将会了解到如何向量化多个训练样本，并计算出结果。

如果有 $m$ 个训练样本，使用非向量化形式的实现就需要重复 $m$ 次以上过程。

$
for \quad i=1 \quad to \quad m:
\\
\qquad z^{[1](i)} = (W^{[1](i)})^{T}a^{[0](i)} + b^{[1](i)}
\\
\qquad a^{[1](i)} = \sigma(z^{[1](i)})
\\
\qquad z^{[2](i)} = (W^{[2](i)})^{T}a^{[1](i)} + b^{[2](i)}
\\
\qquad a^{[2](i)} = \sigma(z^{[2](i)})
$

> 注：$a^{[2](i)}$ 中 $[2]$ 指第二层，$(i)$ 指第 $i$ 个训练样本。

使用向量化表示为：

$
X =  \begin{bmatrix}
\vdots & \vdots & & \vdots \\
x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\
\vdots & \vdots & & \vdots \\
\end{bmatrix}
\\
Z^{[1]} = \begin{bmatrix}
\vdots & \vdots & & \vdots \\
z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)} \\
\vdots & \vdots & & \vdots \\
\end{bmatrix}
\\
A^{[1]} = \begin{bmatrix}
\vdots & \vdots & & \vdots \\
a^{[1](1)} & a^{[1](2)} & \cdots & a^{[1](m)} \\
\vdots & \vdots & & \vdots \\
\end{bmatrix}
\\
\vdots
$

以此类推，从小写的向量 $x$ 到这个大写 $X$ 的矩阵，只是通过组合 $x$ 向量在 $X$ 矩阵的各列中。

同样的，对于 $Z^{[1]}$，$A^{[1]}$，$Z^{[2]}$ 和 $A^{[2]}$，也是这样得到。

从水平上看，矩阵 $A$ 代表了各个训练样本。从竖直上看，矩阵 $A$ 的不同的索引对应于不同的隐藏单元。

对于矩阵 $Z$，$X$ 情况也类似，水平方向上，对应于不同的训练样本；竖直方向上，对应不同的输入特征，而这就是神经网络输入层中各个节点。

最后得到：

$
Z^{[1]} = (W^{[1]})^{T}X + b^{[1]}
\\
A^{[1]} = \sigma(Z^{[1]})
\\
Z^{[2]} = (W^{[2]})^{T}A^{[1]} + b^{[2]}
\\
A^{[2]} = \sigma(Z^{[2]})
$
