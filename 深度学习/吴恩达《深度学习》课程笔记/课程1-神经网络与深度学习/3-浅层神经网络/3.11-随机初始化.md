# 随机初始化

如果在初始时将两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，反向传播通过梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。

在初始化的时候，$W$ 参数要进行随机初始化，不可以设置为 0。而 $b$ 因为不存在对称性的问题，可以设置为 0。

以 2 个输入，2 个隐藏神经元为例：

```python
W = np.random.rand(2,2)* 0.01
b = np.zeros((2,1))
```

这里将 $W$ 的值乘以 0.01（或者其他的常数值）的原因是为了使得权重 $W$ 初始化为较小的值，这是因为使用 $sigmoid$ 函数或者 $tanh$ 函数作为激活函数时，$W$ 比较小，则 $Z=WX+b$ 所得的值趋近于 0，梯度较大，能够提高算法的更新速度。而如果 $W$ 设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。

$ReLU$ 和 $Leaky ReLU$ 作为激活函数时不存在这种问题，因为在大于 0 的时候，梯度均为 1。