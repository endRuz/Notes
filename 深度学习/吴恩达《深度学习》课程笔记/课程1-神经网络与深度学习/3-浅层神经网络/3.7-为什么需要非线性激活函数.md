# 为什么需要非线性激活函数

事实证明：要让你的神经网络能够计算出有趣的函数，你必须使用非线性激活函数，证明如下：

这是神经网络正向传播的方程：

$
\begin{cases}
z^{[1]} = W^{[1]}x + b^{[1]}
\\
a^{[1]} = g^{[1]}(z^{[1]})
\\
z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}
\\
a^{[2]} = g^{[2]}(z^{[2]})
\end{cases}
$

令 $g(z) = z$，这个有时被叫做 **线性激活函数**（更学术点的名字是 **恒等激活函数**，因为它们就是把输入值输出）。

得到

$
\begin{cases}
z^{[1]} = W^{[1]}x + b^{[1]}
\\
a^{[1]} = z^{[1]}
\\
z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}
\\
a^{[2]} = z^{[2]}
\end{cases}
\\
a^{[2]} = W^{[2]}(W^{[1]}x + b^{[1]}) + b^{[2]}
\\
a^{[2]} = (W^{[2]}W^{[1]})x + (W^{[2]}b^{[1]} + b^{[2]})
\\
a^{[2]} = W'x + b'
$

其中 $W' = W^{[2]}W^{[1]}$，$b'= W^{[2]}b^{[1]} + b^{[2]}$。

使用线性激活函数和不使用激活函数、直接使用 Logistic 回归没有区别，那么无论神经网络有多少层，输出都是输入的线性组合，与 **没有隐藏层** 效果相当，就成了最原始的感知器了。
