# 神经网络的梯度下降

- [神经网络的梯度下降](#神经网络的梯度下降)
  - [正向传播（forward propagation）](#正向传播forward-propagation)
  - [反向传播（back propagation）](#反向传播back-propagation)

## 正向传播（forward propagation）

$$
\begin{cases}
Z^{[1]} = W^{[1]}X + b^{[1]}
\\
A^{[1]} = g^{[1]}(Z^{[1]})
\\
Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}
\\
A^{[2]} = g^{[2]}(Z^{[2]})
\end{cases}
$$

## 反向传播（back propagation）

$$
\begin{cases}
dZ^{[2]} = A^{[2]} - Y, Y = [y^{[1]}, y^{[2]}, \cdots,y^{[m]}]
\\
dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]T}
\\
db^{[2]} = \frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)
\\
dZ^{[1]} = \underbrace{W^{[2]T}dZ^{[2]}}_{(n^{[1]}, m)} \ast \underbrace{g^{[1]\prime}}_{activation \, function \, of \, hidden \, layer} \ast \underbrace{z^{[1]}}_{(n^{[1]}, m)}
\\
dW^{[1]} = \frac{1}{m}dZ^{[1]}X^{T}
\\
\underbrace{db^{[1]}}_{(n^{[1]}, 1)} = \frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True)
\end{cases}
$$
