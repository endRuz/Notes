# m 个样本的梯度下降

逻辑回归公式：

$$z=w^{T}x+b$$

$$\hat{y}=a=\sigma(z)=\frac{1}{1+e^{-z}}$$

损失函数：

$$L(a,y) = -(y\log{a} + (1-y)\log{(1-a)})$$

代价函数公式：

$$J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) = - \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} \log{\hat{y}^{(i)}} + (1 - y^{(i)}) \log{(1 - \hat{y}^{(i)})})$$

逻辑回归梯度下降伪代码：

```python
J_MIN = float('inf')

while true:
    # 初始化
    J = 0
    db = 0
    for i in range(m):
        dw_i = 0

    for i in range(m):
        # 逻辑回归公式
        z^i = w^T * x^i + b
        a^i = sigmoid(z^i)
        # 计算损失函数
        L = -[y^i * log(a^i) + (1 - y^i) * log(1 - a^i)]
        # 累加损失函数和参数的导数
        J += L
        dz^i = a^i - y^i
        for j in range(m):
            dw_j += x^i_j * dz^i
        db += dz^i

    # 计算代价函数
    J /= m

    if J <= J_MIN:
        # 更新最小的代价函数
        J_MIN = J
    else:
        # 若为局部最优解结束迭代
        break

    # 计算参数的导数
    for i in range(m):
        dw_i /= m
    db /= m

    # 更新参数
    for i in range(m):
        w_i = w_i - alpha * dw_i
    b = b - alpha * db
```
