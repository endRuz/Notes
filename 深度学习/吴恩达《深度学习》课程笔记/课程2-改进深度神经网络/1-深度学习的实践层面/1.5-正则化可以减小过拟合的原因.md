# 正则化可以减小过拟合的原因

- [正则化可以减小过拟合的原因](#正则化可以减小过拟合的原因)
  - [直观解释](#直观解释)
  - [数学解释](#数学解释)
  - [其他解释](#其他解释)

## 直观解释

正则化因子设置的足够大的情况下，为了使代价函数（Cost Function）最小化，权重矩阵 $W$ 就会被设置为接近于 $0$ 的值，直观上相当于消除了很多神经元的影响，那么大的神经网络就会变成一个较小的网络。当然，实际上隐藏层的神经元依然存在，但是其影响减弱了，便不会导致过拟合。

## 数学解释

假设神经元中使用的激活函数为 `g(z) = tanh(z)`（sigmoid 同理）。

![regularization_prevent_overfitting](image/1.5-1.png)

在加入正则化项后，当 $\lambda$ 增大，导致 $W^{[l]}$ 减小，$Z^{[l]} = W^{[l]} a^{[l−1]} + b^{[l]}$ 便会减小。由上图可知，在 $z$ 较小（接近于 0）的区域里，$tanh(z)$ 函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，因此不会发生过拟合。

## 其他解释

在权值 $w^{[L]}$ 变小之下，输入样本 $X$ 随机的变化不会对神经网络模造成过大的影响，神经网络受局部噪音的影响的可能性变小。这就是正则化能够降低模型方差的原因。
