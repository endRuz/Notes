# 正则化

正则化是在代价函数（Cost Function）中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题。

## Logistic 回归中的正则化

对于 Logistic 回归，加入 L2 正则化（也称 “L2 范数”）的代价函数（Cost Function）：

$$
J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m}||w||_{2}^{2}
$$

- L2 正则化：

$$
\frac{\lambda}{2m}||w||_{2}^{2} = \frac{\lambda}{2m} \sum_{j=1}^{n_{x}} w^{2}_{j} = \frac{\lambda}{2m} w^{T} w
$$

- L1 正则化：

$$
\frac{\lambda}{2m}||w||_{1} = \frac{\lambda}{2m} \sum_{j=1}^{n_{x}} |w_{j}|
$$

其中，$\lambda$ 为 **正则化因子**，是 **超参数**。

由于 L1 正则化最后得到 $w$ 向量中将存在大量的 $0$，使模型变得稀疏化，因此 L2 正则化更加常用。

> 注意，lambda 在 Python 中属于保留字，所以在编程的时候，用 lambd 代替这里的正则化因子。

## 神经网络中的正则化

对于神经网络，加入正则化的代价函数（Cost Function）：

$$
J(w^{[1]}, b^{[1]}, \dots, w^{[L]}, b^{[L]}) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m} \sum_{l=1}^{L} || w^{[l]} ||_{F}^{2}
$$

因为 $w$ 的大小为 $(n^{[l−1]}, n^{[l]})$，因此

$$
|| w^{[l]} ||_{F}^{2} = \sum_{i=1}^{n^{[l-1]}} \sum_{j=1}^{n^{[l]}} (w_{ij}^{[l]})^{2}
$$

该矩阵范数被称为 **弗罗贝尼乌斯范数（Frobenius Norm）**，用下标 $F$ 标注，所以神经网络中的正则化项被称为弗罗贝尼乌斯范数矩阵。

## 权重衰减（Weight decay）

在加入正则化项后，梯度变为（反向传播要按这个计算）：

$$
dW^{[l]} = \frac{\partial L}{\partial w^{[l]}} + \frac{\lambda}{m} W^{[l]}
$$

代入梯度更新公式：

$$
W^{[l]} := W^{[l]} − \alpha dW^{[l]}
$$

可得：

$$
\begin{aligned}
W^{[l]} & := W^{[l]} − \alpha [\frac{\partial L}{\partial w^{[l]}} + \frac{\lambda}{m}W^{[l]}] \\
        &  = W^{[l]} − \alpha \frac{\lambda}{m}W^{[l]} − \alpha \frac{\partial L}{\partial w^{[l]}} \\
        &  = (1 − \frac{\alpha \lambda}{m})W^{[l]} − \alpha \frac{\partial L}{\partial w^{[l]}} \\
\end{aligned}
$$

其中，因为 $1 − \frac{\alpha \lambda}{m} < 1$，会给原来的 $W^{[l]}$ 一个衰减的参数，因此 L2 正则化项也被称为 **权重衰减（Weight Decay）**。
